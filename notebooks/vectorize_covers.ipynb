{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# ðŸ’¿ Album Cover Vectorization\n",
    "\n",
    "Goal: Process album cover images into high-dimensional vectors (embeddings) using SigLIP to enable semantic search in the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../../../data/covertartarchive/bm/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm = pd.read_csv(data_dir / \"df_final_with_covers.csv\")\n",
    "df_bm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm[[\"band\", \"album\", \"cover_url\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bm[\"cover_file\"] = df_bm[\"album_id\"].apply(lambda x: f\"{str(x)[-1]}/{x}.jpg\")\n",
    "df_bm = df_bm.dropna(subset=[\"cover_url\"]).reset_index(drop=True)\n",
    "img_paths = [data_dir / fname for fname in df_bm[\"cover_file\"]]\n",
    "df_bm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = [data_dir / fname for fname in df_bm[\"cover_file\"]]\n",
    "print(sum([p.exists() for p in img_paths]) / len(img_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Embedding covers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Model Initialization\n",
    "\n",
    "We use SigLI via OpenCLIP. This model provides a shared embedding space for text and images, allowing us to find images based on natural language descriptions.\n",
    "\n",
    "Exact implementation: https://huggingface.co/timm/ViT-B-16-SigLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device priority: CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# model_name = \"ViT-SO400M-14-SigLIP\"\n",
    "model_name = \"ViT-B-16-SigLIP\"\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    model_name, pretrained=\"webli\", device=device\n",
    ")\n",
    "# Load tokenizer\n",
    "tokenizer = open_clip.get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Data Loading & Preprocessing\n",
    "\n",
    "Images are loaded and transformed to the specific resolution (224x224) required by the SigLIP Vision Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbumDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A standard PyTorch Dataset for album covers.\n",
    "\n",
    "    Assumes all image files in 'paths' exist and are valid.\n",
    "    Loads images, converts them to RGB, and applies SigLIP preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, paths, preprocess):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            paths (list): List of pathlib.Path objects.\n",
    "            preprocess (callable): The image transformation pipeline.\n",
    "        \"\"\"\n",
    "        self.paths = paths\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            tuple: (preprocessed_tensor, index)\n",
    "        \"\"\"\n",
    "        path = self.paths[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        return self.preprocess(image), idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AlbumDataset(img_paths, preprocess)\n",
    "loader = DataLoader(\n",
    "    dataset, batch_size=32, shuffle=False, num_workers=0, pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Embedding Generation\n",
    "\n",
    "We iterate through the dataset in batches to generate 768-dimensional vectors using the SigLIP encoder.\n",
    "\n",
    "Normalization: All vectors are L2 normalized (default p=2). This ensures their magnitude is 1.0, and that a simple dot product gives the Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = []\n",
    "embeddings = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, indices in tqdm(loader, desc=f\"Encoding on {device}\"):\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        features = model.encode_image(images)\n",
    "\n",
    "        # Normalize\n",
    "        features /= features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        embeddings.append(features.cpu().numpy())\n",
    "        valid_indices.extend(indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final matrix\n",
    "all_embs = np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the column\n",
    "df_bm[\"embedding\"] = None\n",
    "\n",
    "# 2. Convert the matrix into a list of individual arrays\n",
    "# This allows Pandas to place one array into each cell\n",
    "embedding_list = list(all_embs)\n",
    "\n",
    "# 3. Map them back to the correct rows\n",
    "df_bm.loc[valid_indices, \"embedding\"] = pd.Series(list(all_embs), index=valid_indices)\n",
    "\n",
    "# 4. Create a helper boolean for easy filtering later\n",
    "df_bm[\"has_embedding\"] = df_bm[\"embedding\"].notna()\n",
    "\n",
    "\n",
    "# 5. Check how many were mapped\n",
    "total_rows = len(df_bm)\n",
    "mapped_count = df_bm[\"has_embedding\"].sum()\n",
    "# Calculate percentage\n",
    "pct_success = (mapped_count / total_rows) * 100\n",
    "\n",
    "print(\n",
    "    f\"Successfully mapped {mapped_count} embeddings to the DataFrame \"\n",
    "    f\"({pct_success:.2f}% success rate).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Pickle: Preserves NumPy arrays natively and requires no extra dependencies.\n",
    "# File size: ~16MB\n",
    "cols_to_save = [\"band\", \"album\", \"year\", \"cover_url\", \"embedding\", \"album_id\"]\n",
    "df_bm[df_bm[\"has_embedding\"]][cols_to_save].to_pickle(\n",
    "    \"../data/bm_covers_with_embeddings.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Semantic Search Validation\n",
    "\n",
    "Verify SigLIP's ability to retrieve relevant album covers using descriptive text queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"../data/bm_covers_with_embeddings.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_query(text, model, tokenizer, device):\n",
    "    \"\"\"Encode text query using SigLIP-specific tokenizer\"\"\"\n",
    "    text_tokens = tokenizer([text]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        return text_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_albums(query_text, df, model, tokenizer, device, top_k=10):\n",
    "    \"\"\"Search the dataframe and return top matches\"\"\"\n",
    "    # 1. Get query vector\n",
    "    query_emb = encode_text_query(query_text, model, tokenizer, device)\n",
    "\n",
    "    # 2. Extract matrix of valid embeddings [N, 768]\n",
    "    matrix = np.vstack(df[\"embedding\"].values)\n",
    "\n",
    "    # 3. Calculate scores (Dot product = Cosine Similarity for normalized vectors)\n",
    "    scores = (query_emb @ matrix.T).flatten()\n",
    "\n",
    "    # 4. Get top indices\n",
    "    best_indices = scores.argsort()[::-1][:top_k]\n",
    "\n",
    "    # 5. Return the top rows with their scores\n",
    "    results = df.iloc[best_indices].copy()\n",
    "    results[\"search_score\"] = scores[best_indices]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Semantic Search Sandbox ---\n",
    "query = \"a dark castle\"\n",
    "top_10 = find_top_albums(query, df, model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def display_top_3_html(results_df):\n",
    "    html_str = '<div style=\"display: flex; gap: 20px; align-items: flex-start;\">'\n",
    "\n",
    "    for _, row in results_df.head(3).iterrows():\n",
    "        html_str += f\"\"\"\n",
    "        <div style=\"flex: 1; text-align: center; max-width: 250px;\">\n",
    "            <img src=\"{row[\"cover_url\"]}\" style=\"width: 100%; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);\">\n",
    "            <p style=\"margin: 8px 0 0 0; font-weight: bold; font-family: sans-serif;\">{row[\"band\"]}</p>\n",
    "            <p style=\"margin: 2px 0; font-style: italic; font-family: sans-serif;\">{row[\"album\"]}</p>\n",
    "            <p style=\"color: #666; font-size: 0.8em; font-family: sans-serif;\">Score: {row[\"search_score\"]:.3f}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    html_str += \"</div>\"\n",
    "    display(HTML(html_str))\n",
    "\n",
    "\n",
    "display_top_3_html(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit-bm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
